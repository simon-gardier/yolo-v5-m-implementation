{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1485588",
      "metadata": {
        "id": "a1485588"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70cba002",
      "metadata": {
        "id": "70cba002"
      },
      "outputs": [],
      "source": [
        "!pip3 install wandb matplotlib torch torchviz torchvision torchsummary torchviz weave nbformat netron onnx roboflow scikit-learn roboflow netron --quiet\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.utils.data as data\n",
        "from PIL import Image, ImageDraw\n",
        "from torchvision import datasets, transforms, utils\n",
        "import wandb\n",
        "import netron\n",
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b7d184",
      "metadata": {
        "id": "03b7d184"
      },
      "source": [
        "# **Util Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6479481d",
      "metadata": {
        "id": "6479481d"
      },
      "outputs": [],
      "source": [
        "def show_image(img):\n",
        "    plt.imshow(transforms.functional.to_pil_image(img))\n",
        "    plt.show()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, labels\n",
        "\n",
        "def show_image_with_labels(image, labels, class_names=None):\n",
        "    image_np = image.permute(1, 2, 0).numpy()\n",
        "    h, w, _ = image_np.shape\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    for label in labels:\n",
        "        class_id, x_center, y_center, bw, bh = label.tolist()\n",
        "        x = (x_center - bw / 2) * w\n",
        "        y = (y_center - bh / 2) * h\n",
        "        box_w = bw * w\n",
        "        box_h = bh * h\n",
        "        rect = patches.Rectangle((x, y), box_w, box_h, linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        if class_names:\n",
        "            class_text = class_names[int(class_id)]\n",
        "        else:\n",
        "            class_text = str(int(class_id))\n",
        "        ax.text(x, y - 5, class_text, color='white', fontsize=12,bbox=dict(facecolor='red', alpha=0.5, pad=2))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "classes_types = {\n",
        "    0: 'gold_ore',\n",
        "    1: 'iron_ore',\n",
        "    2: 'diamond_ore',\n",
        "    3: 'redstone_ore',\n",
        "    4: 'deepslate_iron_ore'\n",
        "}\n",
        "classes_number = len(classes_types)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57471f90",
      "metadata": {
        "id": "57471f90"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2b4d5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f2b4d5f",
        "outputId": "39212e98-5432-40be-fdb3-51e8d14223f8"
      },
      "outputs": [],
      "source": [
        "# Download dataset only if not already present\n",
        "dataset_dir = \"minecraft-ore-1\"\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"bVvy50uMbWp85HBSLUkm\")\n",
        "project = rf.workspace(\"oblig10\").project(\"minecraft-ore\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")\n",
        "\n",
        "# Remove problematic file\n",
        "file_path_label = \"minecraft-ore-1/valid/labels/2024-04-25_19-38-10_png_jpg.rf.627bb52ce40ad0431564b93df2aa900f.txt\"\n",
        "file_path_image = \"minecraft-ore-1/valid/images/2024-04-25_19-38-10_png_jpg.rf.627bb52ce40ad0431564b93df2aa900f.jpg\"\n",
        "if os.path.exists(file_path_label):\n",
        "  os.remove(file_path_label)\n",
        "  print(f\"Deleted: {file_path_label}\")\n",
        "if os.path.exists(file_path_image):\n",
        "  os.remove(file_path_image)\n",
        "  print(f\"Deleted: {file_path_image}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afac7fa",
      "metadata": {
        "id": "5afac7fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from os import path\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class MinecraftV1(Dataset):\n",
        "    def __init__(self, root, train=True, valid=False, transform=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.valid = valid\n",
        "        self.transform = transform\n",
        "\n",
        "        if train:\n",
        "            self.data_path = path.join(root, 'train')\n",
        "        elif valid:\n",
        "            self.data_path = path.join(root, 'valid')\n",
        "        else:\n",
        "            self.data_path = path.join(root, 'test')\n",
        "\n",
        "        self.images_path = path.join(self.data_path, 'images')\n",
        "        self.labels_path = path.join(self.data_path, 'labels')\n",
        "        self.data_images = []\n",
        "        self.data_labels = []\n",
        "        image_files = sorted(os.listdir(self.images_path))\n",
        "        label_files = sorted(os.listdir(self.labels_path))\n",
        "        for image_file in image_files:\n",
        "            image_path = path.join(self.images_path, image_file)\n",
        "            self.data_images.append(image_path)\n",
        "\n",
        "        for label_file in label_files:\n",
        "            label_path = path.join(self.labels_path,label_file)\n",
        "            self.data_labels.append(label_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.data_images[idx]\n",
        "        label_path = self.data_labels[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        with open(label_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        labels = [list(map(float, line.strip().split())) for line in lines]\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=0.05):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
        "        tensor = tensor + noise\n",
        "        return torch.clamp(tensor, 0., 1.)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
        "\n",
        "basic_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "augmented_transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(0., 0.03)\n",
        "])\n",
        "\n",
        "mc_train = MinecraftV1(root=os.path.join(os.getcwd(), \"minecraft-ore-1\"), transform=augmented_transform)\n",
        "mc_test = MinecraftV1(root=os.path.join(os.getcwd(), \"minecraft-ore-1\"), train=False, transform=basic_transform)\n",
        "mc_valid = MinecraftV1(root=os.path.join(os.getcwd(), \"minecraft-ore-1\"), train=False,valid=True, transform=basic_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e86a5b4",
      "metadata": {
        "id": "6e86a5b4"
      },
      "source": [
        "# **DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2c5e98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3f2c5e98",
        "outputId": "8856c06a-47a9-4536-dd17-961f35d2075c"
      },
      "outputs": [],
      "source": [
        "trainloader = data.DataLoader(mc_train, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "validloader = data.DataLoader(mc_valid, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "testloader = data.DataLoader(mc_test, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "\n",
        "for i in range(4):\n",
        "    show_image_with_labels(images[i], labels[i], class_names=classes_types)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c37134",
      "metadata": {
        "id": "b6c37134"
      },
      "source": [
        "# **Device**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9594731d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9594731d",
        "outputId": "4badcfb4-753b-4bf6-9026-f3c4dbd680e3"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4727c7c",
      "metadata": {
        "id": "d4727c7c"
      },
      "source": [
        "# **Yolo V5**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c45811",
      "metadata": {
        "id": "30c45811"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739c1494",
      "metadata": {
        "id": "739c1494"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Conv-BN-SiLU block\n",
        "class CBS(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None):\n",
        "        super().__init__()\n",
        "        if padding is None:\n",
        "            padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.SiLU()\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.batch_norm(self.conv(x)))\n",
        "\n",
        "# Bottleneck for C3\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, shortcut=True, expansion=0.5):\n",
        "        super().__init__()\n",
        "        hidden_channels = int(out_channels * expansion)\n",
        "        self.conv1 = CBS(in_channels, hidden_channels, 1)\n",
        "        self.conv2 = CBS(hidden_channels, out_channels, 3)\n",
        "        self.use_shortcut = shortcut and in_channels == out_channels\n",
        "    def forward(self, x):\n",
        "        out = self.conv2(self.conv1(x))\n",
        "        return x + out if self.use_shortcut else out\n",
        "\n",
        "# CSP C3 block\n",
        "class C3(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks=1, shortcut=True, expansion=0.5):\n",
        "        super().__init__()\n",
        "        hidden_channels = int(out_channels * expansion)\n",
        "        self.conv1 = CBS(in_channels, hidden_channels, 1)\n",
        "        self.conv2 = CBS(in_channels, hidden_channels, 1)\n",
        "        self.bottlenecks = nn.Sequential(\n",
        "            *[Bottleneck(hidden_channels, hidden_channels, shortcut, expansion) for _ in range(num_blocks)]\n",
        "        )\n",
        "        self.conv3 = CBS(2 * hidden_channels, out_channels, 1)\n",
        "    def forward(self, x):\n",
        "        return self.conv3(torch.cat((self.bottlenecks(self.conv1(x)), self.conv2(x)), dim=1))\n",
        "\n",
        "# SPPF block\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "        super().__init__()\n",
        "        hidden_channels = in_channels // 2\n",
        "        self.conv1 = CBS(in_channels, hidden_channels, 1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
        "        self.conv2 = CBS(hidden_channels * 4, out_channels, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        y1 = self.maxpool(x)\n",
        "        y2 = self.maxpool(y1)\n",
        "        y3 = self.maxpool(y2)\n",
        "        return self.conv2(torch.cat([x, y1, y2, y3], dim=1))\n",
        "\n",
        "# CSPDarknet Backbone\n",
        "class CSPDarknet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = CBS(3, 64, kernel_size=6, stride=2, padding=2)\n",
        "        self.stage2 = nn.Sequential(\n",
        "            CBS(64, 128, 3, 2),\n",
        "            C3(128, 128, num_blocks=2)\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            CBS(128, 256, 3, 2),\n",
        "            C3(256, 256, num_blocks=4)\n",
        "        )\n",
        "        self.stage4 = nn.Sequential(\n",
        "            CBS(256, 512, 3, 2),\n",
        "            C3(512, 512, num_blocks=6)\n",
        "        )\n",
        "        self.stage5 = nn.Sequential(\n",
        "            CBS(512, 1024, 3, 2),\n",
        "            C3(1024, 1024, num_blocks=2),\n",
        "            SPPF(1024, 1024, kernel_size=5)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        feature2 = self.stage2(x)\n",
        "        feature3 = self.stage3(feature2)\n",
        "        feature4 = self.stage4(feature3)\n",
        "        feature5 = self.stage5(feature4)\n",
        "        return feature3, feature4, feature5  # P3/8, P4/16, P5/32\n",
        "\n",
        "# PANet Neck\n",
        "class PANet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # top-down pathway\n",
        "        self.reduce_conv_p5     = CBS(1024, 512, 1)\n",
        "        self.reduce_conv_p4     = CBS(512, 512, 1)\n",
        "        self.c3_topdown_p4      = C3(1024, 512, shortcut=False, num_blocks=2)\n",
        "        self.reduce_conv_p4_td  = CBS(512, 256, 1)\n",
        "        self.reduce_conv_p3     = CBS(256, 256, 1)\n",
        "        self.c3_topdown_p3      = C3(512, 256, shortcut=False, num_blocks=2)\n",
        "        # bottom-up pathway\n",
        "        self.downsample_conv_p3 = CBS(256, 256, 3, 2)\n",
        "        self.c3_bottomup_p4     = C3(256+512, 512,  shortcut=False, num_blocks=1)\n",
        "        self.downsample_conv_p4 = CBS(512, 512, 3, 2)\n",
        "        self.c3_bottomup_p5     = C3(512+512, 1024,  shortcut=False, num_blocks=1)\n",
        "        self.upsample           = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, x3, x4, x5):\n",
        "        # top-down\n",
        "        p5 = self.reduce_conv_p5(x5)\n",
        "        p5_up = self.upsample(p5)\n",
        "        p4 = self.reduce_conv_p4(x4)\n",
        "        p4_td = self.c3_topdown_p4(torch.cat([p4, p5_up], 1))\n",
        "\n",
        "        p4_td_reduced = self.reduce_conv_p4_td(p4_td)\n",
        "        p4_up = self.upsample(p4_td_reduced)\n",
        "        p3 = self.reduce_conv_p3(x3)\n",
        "        p3_td = self.c3_topdown_p3(torch.cat([p3, p4_up], 1))\n",
        "\n",
        "        # bottom-up\n",
        "        p3_down = self.downsample_conv_p3(p3_td)\n",
        "        p4_bu = self.c3_bottomup_p4(torch.cat([p3_down, p4_td], 1))\n",
        "\n",
        "        p4_down = self.downsample_conv_p4(p4_bu)\n",
        "        p5_bu = self.c3_bottomup_p5(torch.cat([p4_down, p5], 1))\n",
        "        return p3_td, p4_bu, p5_bu\n",
        "\n",
        "# Detect Head\n",
        "class Detect(nn.Module):\n",
        "    def __init__(self, num_classes=5, anchors=(), channels=()):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_outputs = num_classes + 5\n",
        "        self.num_layers = len(anchors)\n",
        "        self.num_anchors = len(anchors[0]) // 2\n",
        "        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.num_layers, -1, 2))\n",
        "        self.detect_convs = nn.ModuleList([nn.Conv2d(ch, self.num_outputs * self.num_anchors, 1) for ch in channels])\n",
        "    def forward(self, features):\n",
        "        outputs = []\n",
        "        for i in range(self.num_layers):\n",
        "            pred = self.detect_convs[i](features[i])\n",
        "            batch_size, _, height, width = pred.shape\n",
        "            pred = pred.view(batch_size, self.num_anchors, self.num_outputs, height, width).permute(0, 1, 3, 4, 2).contiguous()\n",
        "            outputs.append(pred)\n",
        "        return outputs\n",
        "\n",
        "# YOLOv5m 6.0\n",
        "class YOLOv5m(nn.Module):\n",
        "    def __init__(self, num_classes=5, anchors=None):\n",
        "        super().__init__()\n",
        "        if anchors is None:\n",
        "            anchors = [\n",
        "                [10, 13, 16, 30, 33, 23],\n",
        "                [30, 61, 62, 45, 59, 119],\n",
        "                [116, 90, 156, 198, 373, 326]\n",
        "            ]\n",
        "        self.backbone = CSPDarknet()\n",
        "        self.neck = PANet()\n",
        "        self.detect = Detect(num_classes, anchors, channels=[256, 512, 1024])\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature_p3, feature_p4, feature_p5 = self.backbone(x)\n",
        "        neck_p3, neck_p4, neck_p5 = self.neck(feature_p3, feature_p4, feature_p5)\n",
        "        return self.detect([neck_p3, neck_p4, neck_p5])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = YOLOv5m(num_classes=classes_number).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Netron visualization\n",
        "x = torch.randn(1, 3, 640, 640).to(device)\n",
        "y = model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e450c6b",
      "metadata": {
        "id": "3e450c6b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def bbox_iou(box1, box2, CIoU=False, eps=1e-7):\n",
        "    \"\"\"\n",
        "    IoU or CIoU between box1 and box2. Boxes are [N, 4] in (x, y, w, h) format (center).\n",
        "    \"\"\"\n",
        "    # Convert to x1y1x2y2\n",
        "    b1_x1, b1_y1 = box1[..., 0] - box1[..., 2] / 2, box1[..., 1] - box1[..., 3] / 2\n",
        "    b1_x2, b1_y2 = box1[..., 0] + box1[..., 2] / 2, box1[..., 1] + box1[..., 3] / 2\n",
        "    b2_x1, b2_y1 = box2[..., 0] - box2[..., 2] / 2, box2[..., 1] - box2[..., 3] / 2\n",
        "    b2_x2, b2_y2 = box2[..., 0] + box2[..., 2] / 2, box2[..., 1] + box2[..., 3] / 2\n",
        "\n",
        "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "    area1 = (b1_x2 - b1_x1).clamp(0) * (b1_y2 - b1_y1).clamp(0)\n",
        "    area2 = (b2_x2 - b2_x1).clamp(0) * (b2_y2 - b2_y1).clamp(0)\n",
        "    union = area1 + area2 - inter + eps\n",
        "    iou = inter / union\n",
        "\n",
        "    if CIoU:\n",
        "        # center distance\n",
        "        c2 = (torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)) ** 2 + \\\n",
        "             (torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)) ** 2\n",
        "        rho2 = (box1[..., 0] - box2[..., 0]) ** 2 + (box1[..., 1] - box2[..., 1]) ** 2\n",
        "        c2 = c2 + eps\n",
        "\n",
        "        # aspect ratio\n",
        "        v = (4 / (math.pi ** 2)) * torch.pow(torch.atan(box1[..., 2] / (box1[..., 3] + eps)) - torch.atan(box2[..., 2] / (box2[..., 3] + eps)), 2)\n",
        "        with torch.no_grad():\n",
        "            alpha = v / (1 - iou + v + eps)\n",
        "        ciou = iou - (rho2 / c2) - alpha * v\n",
        "        return ciou\n",
        "    return iou\n",
        "\n",
        "def build_targets(p, targets, anchors, device):\n",
        "    \"\"\"\n",
        "    Builds training targets for YOLOv5 loss computation.\n",
        "    Args:\n",
        "        p (list): List of prediction tensors for each detection scale.\n",
        "        targets (list): List of ground truth label tensors for each image in the batch.\n",
        "        anchors (list): List of anchor arrays for each scale.\n",
        "        num_classes (int): Number of classes.\n",
        "        device (torch.device): Device to use for tensor operations.\n",
        "    Returns:\n",
        "        tcls (list): List of class target tensors for each scale.\n",
        "        tbox (list): List of box target tensors for each scale.\n",
        "        indices (list): List of tuples containing indices for each scale.\n",
        "        anch (list): List of anchor tensors for each scale.\n",
        "    \"\"\"\n",
        "    na = 3  # anchors per scale\n",
        "    nl = len(anchors)\n",
        "    tcls, tbox, indices, anch = [], [], [], []\n",
        "    gain = torch.ones(7, device=device)  # normalized to gridspace gain\n",
        "\n",
        "    # Convert targets to (image, class, x, y, w, h)\n",
        "    targets = [torch.cat([torch.full((l.size(0), 1), i, device=device), l], 1) for i, l in enumerate(targets)]\n",
        "    targets = torch.cat(targets, 0) if len(targets) else torch.zeros((0, 6), device=device)\n",
        "    if targets.numel() == 0:\n",
        "        for i in range(nl):\n",
        "            tcls.append(torch.zeros(0, device=device, dtype=torch.long))\n",
        "            tbox.append(torch.zeros(0, 4, device=device))\n",
        "            indices.append((torch.zeros(0, dtype=torch.long),)*4)\n",
        "            anch.append(torch.zeros(0, 2, device=device))\n",
        "        return tcls, tbox, indices, anch\n",
        "\n",
        "    # Append anchor indices to targets\n",
        "    ai = torch.arange(na, device=device).float().view(na, 1).repeat(1, targets.shape[0])\n",
        "    targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # (na, nt, 7)\n",
        "    targets = targets.view(-1, 7)\n",
        "\n",
        "    g = 0.5  # bias\n",
        "    off = torch.tensor([[0, 0], [1, 0], [0, 1], [-1, 0], [0, -1]], device=device).float() * g\n",
        "\n",
        "    for i in range(nl):\n",
        "        anchors_i = torch.tensor(anchors[i], device=device).float().view(na, 2)\n",
        "        gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]\n",
        "        t = targets.clone()\n",
        "        t[:, 2:6] *= gain[2:6]  # scale to grid\n",
        "\n",
        "        # Match targets to anchors\n",
        "        r = t[:, 4:6] / anchors_i[t[:, 6].long()]\n",
        "        j = torch.max(r, 1. / r).max(1)[0] < 4.0  # anchor_t=4.0\n",
        "        t = t[j]\n",
        "\n",
        "        # Offsets\n",
        "        gxy = t[:, 2:4]\n",
        "        gxi = gain[[2, 3]] - gxy\n",
        "        j, k = ((gxy % 1 < g) & (gxy > 1)).T\n",
        "        l, m = ((gxi % 1 < g) & (gxi > 1)).T\n",
        "        j = torch.stack((torch.ones_like(j), j, k, l, m))\n",
        "        t = t.repeat((5, 1, 1))[j]\n",
        "        offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n",
        "\n",
        "        # Define\n",
        "        bc = t[:, :2].long()\n",
        "        gxy = t[:, 2:4]\n",
        "        gwh = t[:, 4:6]\n",
        "        a = t[:, 6].long()\n",
        "        gij = (gxy - offsets).long()\n",
        "        gi, gj = gij.T\n",
        "\n",
        "        b, c = bc.T\n",
        "        indices.append((b, a, gj.clamp_(0, p[i].shape[2] - 1), gi.clamp_(0, p[i].shape[3] - 1)))\n",
        "        tbox.append(torch.cat((gxy - gij, gwh), 1))\n",
        "        anch.append(anchors_i[a])\n",
        "        tcls.append(c)\n",
        "    return tcls, tbox, indices, anch\n",
        "\n",
        "def compute_loss(p, targets, anchors, num_classes, device):\n",
        "    \"\"\"\n",
        "    Compute YOLOv5 loss: box (CIoU), obj (BCE), cls (BCE).\n",
        "    Args:\n",
        "        p: list of predictions for each scale\n",
        "        targets: list of batch label tensors\n",
        "        anchors: list of anchor arrays for each scale\n",
        "        num_classes: number of classes\n",
        "        device: torch.device\n",
        "    Returns:\n",
        "        total_loss, lobj, lbox, lcls\n",
        "    \"\"\"\n",
        "    BCEcls = torch.nn.BCEWithLogitsLoss()\n",
        "    BCEobj = torch.nn.BCEWithLogitsLoss()\n",
        "    lcls = torch.zeros(1, device=device)\n",
        "    lbox = torch.zeros(1, device=device)\n",
        "    lobj = torch.zeros(1, device=device)\n",
        "    tcls, tbox, indices, anch = build_targets(p, targets, anchors, num_classes, device)\n",
        "    for i, pi in enumerate(p):\n",
        "        b, a, gj, gi = indices[i]\n",
        "        tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=device)\n",
        "        n = b.shape[0]\n",
        "        if n:\n",
        "            # Gather predictions\n",
        "            ps = pi[b, a, gj, gi]\n",
        "            # Decode box\n",
        "            pxy = ps[:, :2].sigmoid() * 2 - 0.5\n",
        "            pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anch[i]\n",
        "            pbox = torch.cat((pxy, pwh), 1)\n",
        "            iou = bbox_iou(pbox, tbox[i], CIoU=True)\n",
        "            lbox += (1.0 - iou).mean()\n",
        "            # Objectness\n",
        "            iou = iou.detach().clamp(0)\n",
        "            tobj[b, a, gj, gi] = iou\n",
        "            # Classification\n",
        "            if num_classes > 1:\n",
        "                t = torch.full_like(ps[:, 5:], 0.0, device=device)\n",
        "                t[range(n), tcls[i]] = 1.0\n",
        "                lcls += BCEcls(ps[:, 5:], t)\n",
        "        lobj += BCEobj(pi[..., 4], tobj)\n",
        "    total_loss = lbox + lobj + lcls\n",
        "    return total_loss, lobj, lbox, lcls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f169e727",
      "metadata": {
        "id": "f169e727"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a703426d",
      "metadata": {
        "id": "a703426d"
      },
      "outputs": [],
      "source": [
        "model = YOLOv5m(num_classes=classes_number).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73677f9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "73677f9b",
        "outputId": "192f2c52-e30c-4dae-e2d3-31104fb26646"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "anchors = [\n",
        "    [10, 13, 16, 30, 33, 23],\n",
        "    [30, 61, 62, 45, 59, 119],\n",
        "    [116, 90, 156, 198, 373, 326]\n",
        "]\n",
        "train_array = []\n",
        "valid_array = []\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=\"s-gardier-work\",\n",
        "    project=\"yolov5\",\n",
        "    config={\n",
        "        \"architecture\": \"YOLOv5\",\n",
        "        \"dataset\": \"https://universe.roboflow.com/oblig10/minecraft-ore/dataset/1\",\n",
        "        \"epochs\": num_epochs,\n",
        "    },\n",
        ")\n",
        "\n",
        "anchors = [np.array(a).reshape(-1, 2) for a in anchors]\n",
        "min_validation_loss = np.inf\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        images = images.to(device)\n",
        "        labels = [label.to(device) for label in labels]\n",
        "        outputs = model(images)\n",
        "        total_loss, lobj, lbox, lcls = compute_loss(\n",
        "            outputs, labels, anchors, classes_number, device\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += total_loss.item()\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}\")\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    train_array.append(epoch_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(validloader):\n",
        "            images = images.to(device)\n",
        "            labels = [label.to(device) for label in labels]\n",
        "            outputs = model(images)\n",
        "            total_loss, lobj, lbox, lcls = compute_loss(\n",
        "                outputs, labels, anchors, classes_number, device\n",
        "            )\n",
        "            val_running_loss += total_loss.item()\n",
        "    print(f\"Validation Loss: {val_running_loss / len(validloader):.4f}\")\n",
        "    valid_epoch_loss = val_running_loss / len(validloader)\n",
        "    valid_array.append(valid_epoch_loss)\n",
        "\n",
        "    if min_validation_loss > val_running_loss:\n",
        "        min_validation_loss = val_running_loss\n",
        "        print(f\"Validation loss improved to {min_validation_loss/len(validloader):.4f}, saving model...\")\n",
        "        torch.save(model.state_dict(), f\"yolov5_minecraft_ore_{epoch + 1}.pth\")\n",
        "\n",
        "    run.log({\n",
        "        \"epoch\": (epoch + 1) / num_epochs,\n",
        "        \"training_loss\": running_loss / len(trainloader),\n",
        "        \"validation_loss\": val_running_loss / len(validloader)\n",
        "    })\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b71bc07d",
      "metadata": {
        "id": "b71bc07d"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61847438",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model, model_path):\n",
        "    \"\"\"Load a model from a .pth file.\"\"\"\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def decode_predictions(outputs, confidence_thresh=0.1, iou_thresh=0.5):\n",
        "    \"\"\"Convert model outputs to usable bounding boxes\"\"\"\n",
        "    all_boxes = []\n",
        "    anchors = model.detect.anchors.cpu().numpy()\n",
        "    strides = [8, 16, 32]  # For 640x640 input\n",
        "\n",
        "    for scale_idx, output in enumerate(outputs):\n",
        "        # Convert to numpy and get dimensions\n",
        "        output = output.sigmoid().cpu().detach().numpy()\n",
        "        bs, num_anchors, h, w, _ = output.shape\n",
        "\n",
        "        # Convert dimensions to integers\n",
        "        h = int(h)\n",
        "        w = int(w)\n",
        "\n",
        "        # Get parameters for this scale\n",
        "        stride = strides[scale_idx]\n",
        "        anchor = anchors[scale_idx]\n",
        "\n",
        "        # Create grid\n",
        "        grid_y, grid_x = np.mgrid[:h, :w]\n",
        "\n",
        "        # Reshape output for vectorized operations\n",
        "        output = output.reshape(bs, num_anchors, h, w, -1)\n",
        "\n",
        "        # Decode predictions using vectorized operations\n",
        "        tx = output[..., 0]\n",
        "        ty = output[..., 1]\n",
        "        tw = output[..., 2]\n",
        "        th = output[..., 3]\n",
        "        obj = output[..., 4]\n",
        "        cls_probs = output[..., 5:]\n",
        "\n",
        "        # Calculate absolute coordinates\n",
        "        x = (grid_x + tx) * stride\n",
        "        y = (grid_y + ty) * stride\n",
        "        anchor_w = torch.from_numpy(anchor[:, 0].reshape(1, -1, 1, 1)).float()\n",
        "        anchor_h = torch.from_numpy(anchor[:, 1].reshape(1, -1, 1, 1)).float()\n",
        "        tw_tensor = torch.from_numpy(tw)\n",
        "        th_tensor = torch.from_numpy(th)\n",
        "        w = anchor_w * (torch.sigmoid(tw_tensor) * 2) ** 2\n",
        "        h = anchor_h * (torch.sigmoid(th_tensor) * 2) ** 2\n",
        "        w = w.numpy()\n",
        "        h = h.numpy()\n",
        "\n",
        "        # Calculate class confidence\n",
        "        class_ids = np.argmax(cls_probs, axis=-1)\n",
        "        class_conf = np.take_along_axis(cls_probs, class_ids[..., None], axis=-1).squeeze(-1)\n",
        "        confidence = obj * class_conf\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        mask = confidence > confidence_thresh\n",
        "        for batch_idx in range(bs):\n",
        "            batch_mask = mask[batch_idx]\n",
        "            batch_boxes = np.stack([\n",
        "                x[batch_idx][batch_mask] - w[batch_idx][batch_mask]/2,\n",
        "                y[batch_idx][batch_mask] - h[batch_idx][batch_mask]/2,\n",
        "                x[batch_idx][batch_mask] + w[batch_idx][batch_mask]/2,\n",
        "                y[batch_idx][batch_mask] + h[batch_idx][batch_mask]/2,\n",
        "                confidence[batch_idx][batch_mask],\n",
        "                class_ids[batch_idx][batch_mask]\n",
        "            ], axis=-1)\n",
        "\n",
        "            if batch_boxes.size > 0:\n",
        "                all_boxes.extend(batch_boxes.tolist())\n",
        "\n",
        "    # Non-Maximum Suppression\n",
        "    if not all_boxes:\n",
        "        return []\n",
        "\n",
        "    boxes = np.array(all_boxes)\n",
        "    x1, y1, x2, y2, scores, class_ids = boxes.T\n",
        "\n",
        "    # Calculate areas and sort\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "    keep = []\n",
        "\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "\n",
        "        # Calculate overlaps\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        intersection = w * h\n",
        "\n",
        "        iou = intersection / (areas[i] + areas[order[1:]] - intersection)\n",
        "\n",
        "        # Filter boxes\n",
        "        inds = np.where(iou <= iou_thresh)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return boxes[keep].tolist()\n",
        "\n",
        "def test_single_image(model, dataset, index=0):\n",
        "    # Get image and labels\n",
        "    image, true_labels = dataset[index]\n",
        "    true_labels = true_labels.cpu().numpy()\n",
        "\n",
        "    # Run inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image.unsqueeze(0).to(device))\n",
        "\n",
        "    # Decode predictions\n",
        "    pred_boxes = decode_predictions(outputs)\n",
        "    print(pred_boxes)\n",
        "\n",
        "    # Convert true labels to box format\n",
        "    true_boxes = []\n",
        "    img_w, img_h = 640, 640  # Our image size\n",
        "    for label in true_labels:\n",
        "        class_id, xc, yc, bw, bh = label\n",
        "        x = (xc - bw/2) * img_w\n",
        "        y = (yc - bh/2) * img_h\n",
        "        w = bw * img_w\n",
        "        h = bh * img_h\n",
        "        true_boxes.append([x, y, x+w, y+h, 1.0, class_id])\n",
        "\n",
        "    # Visualize\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    # Draw true boxes (green)\n",
        "    for box in true_boxes:\n",
        "        x1, y1, x2, y2, _, class_id = box\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1), x2-x1, y2-y1,\n",
        "            linewidth=2, edgecolor='lime', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1-5, classes_types[int(class_id)],\n",
        "                color='white', fontsize=10,\n",
        "                bbox=dict(facecolor='lime', alpha=0.8, pad=1))\n",
        "\n",
        "    # Draw predicted boxes (red)\n",
        "    for box in pred_boxes:\n",
        "        x1, y1, x2, y2, conf, class_id = box\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1), x2-x1, y2-y1,\n",
        "            linewidth=2, edgecolor='red', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1-5, f\"{classes_types[int(class_id)]} {conf:.2f}\",\n",
        "                color='white', fontsize=10,\n",
        "                bbox=dict(facecolor='red', alpha=1, pad=1))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a754bdaa",
      "metadata": {
        "id": "a754bdaa"
      },
      "outputs": [],
      "source": [
        "model_path = \"yolov5_minecraft_ore_100.pth\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Error: Model file '{model_path}' does not exist.\")\n",
        "model_from_disk = load_model(model, model_path)\n",
        "\n",
        "random_index = random.randint(0, len(mc_test) - 1)\n",
        "test_single_image(model_from_disk, mc_test, index=random_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c20aea",
      "metadata": {
        "id": "34c20aea"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdf604e",
      "metadata": {
        "id": "8fdf604e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_model(model, dataloader, class_names, iou_thresh=0.5, conf_thresh=0.1):\n",
        "    model.eval()\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "        batch_size = images.size(0)\n",
        "        for i in range(batch_size):\n",
        "            # True labels for this image\n",
        "            true = labels[i].cpu().numpy()\n",
        "            true_classes = true[:, 0].astype(int) if len(true) > 0 else np.array([], dtype=int)\n",
        "            all_true.append(true_classes)\n",
        "\n",
        "            # Predicted boxes for this image\n",
        "            pred_boxes = decode_predictions([out[i:i+1] for out in outputs], confidence_thresh=conf_thresh, iou_thresh=iou_thresh)\n",
        "            pred_classes = np.array([int(box[5]) for box in pred_boxes]) if len(pred_boxes) > 0 else np.array([], dtype=int)\n",
        "            all_pred.append(pred_classes)\n",
        "\n",
        "    # Flatten all_true and all_pred for each class\n",
        "    metrics = {}\n",
        "    for class_id, class_name in class_names.items():\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for t, p in zip(all_true, all_pred):\n",
        "            # For each image, mark 1 if class present, else 0\n",
        "            y_true.append(int(class_id in t))\n",
        "            y_pred.append(int(class_id in p))\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        metrics[class_name] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    for class_name, vals in metrics.items():\n",
        "        print(f\"{class_name}: Precision={vals['precision']:.3f}, Recall={vals['recall']:.3f}, F1={vals['f1']:.3f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate_model(model, testloader, classes_types)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "info8010",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
